{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ -> True, GPU ì‚¬ìš© ë¶ˆê°€ -> False\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import random\n",
    "import easydict\n",
    "import tarfile\n",
    "\n",
    "from tqdm import notebook\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed(df) :\n",
    "    diff_train = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().shift(-1)\n",
    "    diff_train = diff_train['Timestamp'].apply(lambda x : x.total_seconds())\n",
    "    df['elapsed'] = diff_train\n",
    "    \n",
    "    df.groupby('userID').apply(lambda x :x.iloc[:-1])\n",
    "\n",
    "    # í•œ ì‹œê°„ì´ ì§€ë‚˜ë©´ outlierë¡œ ì²˜ë¦¬\n",
    "    outlier = 1*3600\n",
    "    non_outlier = df[df['elapsed'] <= outlier]\n",
    "    # outlierì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” rowë¡œ ì¬êµ¬ì„± í•œ í›„ ê° íƒœê·¸ì˜ í‰ê· ì²˜ë¦¬\n",
    "    mean_elapsed = non_outlier.groupby('KnowledgeTag')['elapsed'].mean()\n",
    "    df.loc[df['elapsed'] > outlier, 'elapsed'] = df[df['elapsed'] > outlier].apply(lambda x: mean_elapsed.get(x['KnowledgeTag'], x['elapsed']), axis=1)\n",
    "    df['elapsed'] = df['elapsed'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def cumsum(df) :\n",
    "    # ëˆ„ì í•©\n",
    "    _cumsum = df.loc[:, ['userID', 'answerCode']].groupby('userID').agg({'answerCode': 'cumsum'})\n",
    "    # ëˆ„ì ê°¯ìˆ˜\n",
    "    _cumcount = df.loc[:, ['userID', 'answerCode']].groupby('userID').agg({'answerCode': 'cumcount'}) + 1\n",
    "\n",
    "    cum_ans = _cumsum / _cumcount\n",
    "    df['cumulative'] = cum_ans['answerCode']\n",
    "\n",
    "    df['paper_number'] = df['assessmentItemID'].apply(lambda x: x[7:]) # assessmentItemIDì˜ ë’¤ì— 3ìë¦¬ë¥¼ ì˜ë¯¸ -> ê° ì‹œí—˜ì§€ ë³„ë¡œ ë¬¸ì œë²ˆí˜¸\n",
    "    # item ì—´ì„ int16ìœ¼ë¡œ ë³€ê²½\n",
    "    df[\"paper_number\"] = df[\"paper_number\"].astype(\"int16\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def avg_percent(x) :\n",
    "    return np.sum(x) / len(x)\n",
    "\n",
    "def type_percent(df) :\n",
    "    # ìœ„ì—ì„œ ì²˜ë¦¬í•œ typeì„ ë³€í™˜í•˜ì—¬ ê°ê°ì˜ ì •ë‹µë¥  ì²˜ë¦¬\n",
    "\n",
    "    df['KnowledgeTag_percent'] = df.groupby('KnowledgeTag')['answerCode'].transform(avg_percent)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.data_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df):\n",
    "        #con_colì— ëŒ€í•œ ì „ì²˜ë¦¬\n",
    "        ###TODO: con_colì— ëŒ€í•œ ì „ì²˜ë¦¬ ì½”ë“œ ì¶”ê°€\n",
    "        con_cols= [\"elapsed\", \"KnowledgeTag_percent\", \"cumulative\", \"paper_number\"]\n",
    "        df = elapsed(df)\n",
    "        df = cumsum(df)\n",
    "        df = type_percent(df)\n",
    "        #################################CUSTUM#############################################\n",
    "        \n",
    "        #cate_colì— ëŒ€í•œ ì „ì²˜ë¦¬\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "        for col in cate_cols:\n",
    "\n",
    "            #For UNKNOWN class\n",
    "            #ë§ˆì§€ë§‰ì„ nanê°’ìœ¼ë¡œ ì¤€ ì´ìœ ëŠ” ë§ˆìŠ¤í‚¹ ë•Œë¬¸ì´ë¼ê³  ìƒê°\n",
    "            a = df[col].unique().tolist() + [np.nan]\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(a)\n",
    "            df[col] = le.transform(df[col])\n",
    "            self.__save_labels(le, col)\n",
    "\n",
    "        # def convert_time(s):\n",
    "        #     timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "        #     return int(timestamp)\n",
    "\n",
    "        # df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name):\n",
    "       \n",
    "        #################custun#########################\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        ######################################################\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path,dtype=dtype, parse_dates=['Timestamp'])\n",
    "        df = self.__preprocessing(df)\n",
    "        print(\"elapsed nanê°’ì˜ ê°œìˆ˜:\",df[\"elapsed\"].isna().sum())\n",
    "        print(\"KnowledgeTag_percent nanê°’ì˜ ê°œìˆ˜:\",df[\"KnowledgeTag_percent\"].isna().sum())\n",
    "        print(\"cumulative nanê°’ì˜ ê°œìˆ˜:\",df[\"cumulative\"].isna().sum())\n",
    "        # ì¶”í›„ featureë¥¼ embeddingí•  ì‹œì— embedding_layerì˜ input í¬ê¸°ë¥¼ ê²°ì •í• ë•Œ ì‚¬ìš©\n",
    "        self.args.n_questions = df['assessmentItemID'].nunique()\n",
    "        self.args.n_test = df['testId'].nunique()\n",
    "        self.args.n_tag = df['KnowledgeTag'].nunique() \n",
    "        \n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "        #ê¸°ì¡´ columns\n",
    "        #columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag']\n",
    "        # group = df[columns].groupby('userID').apply(\n",
    "        #     lambda r: (\n",
    "        #         r['testId'].values,\n",
    "        #         r['assessmentItemID'].values,\n",
    "        #         r['KnowledgeTag'].values,\n",
    "        #         r['answerCode'].values\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        #columns ì¶”ê°€\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag',\"elapsed\", \"KnowledgeTag_percent\", \"cumulative\", \"paper_number\"]\n",
    "\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values,\n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values,\n",
    "                    r['elapsed'].values,\n",
    "                    r['KnowledgeTag_percent'].values,\n",
    "                    r['cumulative'].values,\n",
    "                    r['paper_number'].values,\n",
    "                )\n",
    "            )\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # ê° dataì˜ sequence length\n",
    "        seq_len = len(row[0])\n",
    "        \n",
    "        #original\n",
    "        #test, question, tag, correct = row[0], row[1], row[2], row[3]\n",
    "        #custum\n",
    "        test, question, tag, correct, elapsed, KnowledgeTag_percent, cumulative, paper_number = row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "\n",
    "        cate_cols = [test, question, tag, correct]\n",
    "        \n",
    "        #custum\n",
    "        cont_cols = [elapsed, KnowledgeTag_percent, cumulative, paper_number]\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            # cate_col\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "            # cont_col\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[:seq_len] = 1\n",
    "        \n",
    "        \n",
    "        # max seq lenì„ ê³ ë ¤í•˜ì—¬ì„œ ì´ë³´ë‹¤ ê¸¸ë©´ ìë¥´ê³  ì•„ë‹ ê²½ìš° ê·¸ëŒ€ë¡œ ëƒ…ë‘”ë‹¤\n",
    "        # if seq_len > self.args.max_seq_len:\n",
    "        #     for i, col in enumerate(cate_cols):\n",
    "        #         cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "        #     mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        # else:\n",
    "        #     mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "        #     mask[:seq_len] = 1\n",
    "\n",
    "        # maskë„ columns ëª©ë¡ì— í¬í•¨ì‹œí‚´\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        #custum\n",
    "        cate_cont_cols = []\n",
    "        cate_cont_cols.extend(cate_cols)\n",
    "        cate_cont_cols.extend(cont_cols)\n",
    "        ############################################\n",
    "        #original\n",
    "        # np.array -> torch.tensor í˜•ë³€í™˜\n",
    "        # for i, col in enumerate(cate_cols):\n",
    "        #     cate_cols[i] = torch.tensor(col)\n",
    "        # return cate_cols\n",
    "        #custum\n",
    "        for i, col in enumerate(cate_cont_cols):\n",
    "            cate_cont_cols[i] = torch.tensor(col)\n",
    "        return cate_cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#paddingì„ ìœ„í•œ í•¨ìˆ˜\n",
    "def collate(batch):\n",
    "    col_n = len(batch[0])\n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    # print(\"column ê°œìˆ˜\",col_n)\n",
    "    # batchì˜ ê°’ë“¤ì„ ê° columnë¼ë¦¬ ê·¸ë£¹í™”\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row):\n",
    "            col_list[i].append(col)\n",
    "\n",
    "    # ê° columnì˜ ê°’ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ padding ì§„í–‰\n",
    "    # pad_sequence([[1, 2, 3], [3, 4]]) -> [[1, 2, 3],\n",
    "    #                                       [3, 4, 0]]\n",
    "    for i, col_batch in enumerate(col_list):\n",
    "        col_list[i] = pad_sequence(col_batch, batch_first=True)\n",
    "\n",
    "    # maskì˜ ê²½ìš° max_seq_lenì„ ê¸°ì¤€ìœ¼ë¡œ ê¸¸ì´ê°€ ì„¤ì •ë˜ì–´ìˆë‹¤.\n",
    "    # ë§Œì•½ ë‹¤ë¥¸ columnë“¤ì˜ seq_lenì´ max_seq_lenë³´ë‹¤ ì‘ë‹¤ë©´\n",
    "    # ì´ ê¸¸ì´ì— ë§ì¶”ì–´ maskì˜ ê¸¸ì´ë„ ì¡°ì ˆí•´ì¤€ë‹¤\n",
    "    col_seq_len = col_list[0].size(1)\n",
    "    mask_seq_len = col_list[-1].size(1)\n",
    "    if col_seq_len < mask_seq_len:\n",
    "        col_list[-1] = col_list[-1][:, :col_seq_len]\n",
    "\n",
    "    return tuple(col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "\n",
    "    trainset = DKTDataset(train, args)\n",
    "    valset = DKTDataset(valid, args)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, shuffle=True,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               pin_memory=pin_memory,\n",
    "                                               collate_fn=collate)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valset, shuffle=False,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               pin_memory=pin_memory,\n",
    "                                               collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slidding_window(data, args):\n",
    "    window_size = args.max_seq_len\n",
    "    stride = args.stride\n",
    "\n",
    "    augmented_datas = []\n",
    "    for row in data:\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        # ë§Œì•½ window í¬ê¸°ë³´ë‹¤ seq lenì´ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ augmentationì„ í•˜ì§€ ì•ŠëŠ”ë‹¤\n",
    "        if seq_len <= window_size:\n",
    "            augmented_datas.append(row)\n",
    "        else:\n",
    "            total_window = ((seq_len - window_size) // stride) + 1\n",
    "\n",
    "            # ì•ì—ì„œë¶€í„° slidding window ì ìš©\n",
    "            for window_i in range(total_window):\n",
    "                # windowë¡œ ì˜ë¦° ë°ì´í„°ë¥¼ ëª¨ìœ¼ëŠ” ë¦¬ìŠ¤íŠ¸\n",
    "                window_data = []\n",
    "                for col in row:\n",
    "                    window_data.append(col[window_i*stride:window_i*stride + window_size])\n",
    "\n",
    "                # Shuffle\n",
    "                # ë§ˆì§€ë§‰ ë°ì´í„°ì˜ ê²½ìš° shuffleì„ í•˜ì§€ ì•ŠëŠ”ë‹¤\n",
    "                if args.shuffle and window_i + 1 != total_window:\n",
    "                    shuffle_datas = shuffle(window_data, window_size, args)\n",
    "                    augmented_datas += shuffle_datas\n",
    "                else:\n",
    "                    augmented_datas.append(tuple(window_data))\n",
    "\n",
    "            # slidding windowì—ì„œ ë’·ë¶€ë¶„ì´ ëˆ„ë½ë  ê²½ìš° ì¶”ê°€\n",
    "            total_len = window_size + (stride * (total_window - 1))\n",
    "            if seq_len != total_len:\n",
    "                window_data = []\n",
    "                for col in row:\n",
    "                    window_data.append(col[-window_size:])\n",
    "                augmented_datas.append(tuple(window_data))\n",
    "\n",
    "\n",
    "    return augmented_datas\n",
    "\n",
    "def shuffle(data, data_size, args):\n",
    "    shuffle_datas = []\n",
    "    for i in range(args.shuffle_n):\n",
    "        # shuffle íšŸìˆ˜ë§Œí¼ windowë¥¼ ëœë¤í•˜ê²Œ ê³„ì† ì„ì–´ì„œ ë°ì´í„°ë¡œ ì¶”ê°€\n",
    "        shuffle_data = []\n",
    "        random_index = np.random.permutation(data_size)\n",
    "        for col in data:\n",
    "            shuffle_data.append(col[random_index])\n",
    "        shuffle_datas.append(tuple(shuffle_data))\n",
    "    return shuffle_datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(data, args):\n",
    "    if args.window == True:\n",
    "        data = slidding_window(data, args)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # input embedding\n",
    "        pe = torch.zeros(max_len, d_model) ## max_len X hidden_dim\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #0ë¶€í„° sequence ê¸¸ì´ë§Œí¼ position ê°’ ìƒì„±, 1 X max_len\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Saint(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Saint, self).__init__()\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        # self.dropout = self.args.dropout\n",
    "        self.dropout = 0.\n",
    "\n",
    "        ### Embedding\n",
    "        # ENCODER embedding\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # encoder combination projection\n",
    "        # original\n",
    "        # self.enc_comb_proj = nn.Linear((self.hidden_dim//3)*3, self.hidden_dim)\n",
    "        # custum\n",
    "        self.enc_cate_comb_proj = nn.Linear((self.hidden_dim//3)*3, self.hidden_dim//2)\n",
    "        self.enc_cont_comb_proj = nn.Linear(2, self.hidden_dim//2) ## ì„ì‹œë¡œ í˜„ì¬ 3ê°œë¡œ ì§€ì •, ì¶”í›„ ì½”ë“œ ë³€ê²½ í•„ìš”, cont columnì˜ ê°œìˆ˜ì„\n",
    "        # batchnorm ì¶”ê°€\n",
    "        # self.cont_bn = nn.BatchNorm1d(3)\n",
    "        # # ì¬ìˆ˜ì •\n",
    "        # self.enc_cate_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear((self.hidden_dim//3)*3, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        # self.enc_cont_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear(3, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        ###########################################################################\n",
    "        \n",
    "        # DECODER embedding\n",
    "        # interactionì€ í˜„ì¬ correctìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "\n",
    "        # decoder combination projection\n",
    "        # original\n",
    "        # self.dec_comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "        # custum\n",
    "        self.dec_cate_comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim//2)\n",
    "        self.dec_cont_comb_proj = nn.Linear(2, self.hidden_dim//2)## ì„ì‹œë¡œ í˜„ì¬ 3ê°œë¡œ ì§€ì •, ì¶”í›„ ì½”ë“œ ë³€ê²½ í•„ìš”, cont columnì˜ ê°œìˆ˜ì„\n",
    "        # ì¬ìˆ˜ì •\n",
    "        # self.dec_cate_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear((self.hidden_dim//3)*4, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        # self.dec_cont_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear(3, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        ###########################################################################\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(self.hidden_dim, self.dropout, self.args.max_seq_len)\n",
    "        self.pos_decoder = PositionalEncoding(self.hidden_dim, self.dropout, self.args.max_seq_len)\n",
    "        # cate dataì—ë§Œ positional encoding ì ìš©í•˜ëŠ” ì½”ë“œë¡œ ì„ì‹œ ìˆ˜ì •\n",
    "        # self.pos_encoder = PositionalEncoding(self.hidden_dim//2, self.dropout, self.args.max_seq_len)\n",
    "        # self.pos_decoder = PositionalEncoding(self.hidden_dim//2, self.dropout, self.args.max_seq_len)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=self.args.n_heads,\n",
    "            num_encoder_layers=self.args.n_layers,\n",
    "            num_decoder_layers=self.args.n_layers,\n",
    "            dim_feedforward=self.hidden_dim,\n",
    "            dropout=self.dropout,\n",
    "            activation='relu')\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        self.enc_mask = None\n",
    "        self.dec_mask = None\n",
    "        self.enc_dec_mask = None\n",
    "\n",
    "    def get_mask(self, seq_len):\n",
    "        mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1))\n",
    "\n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #original\n",
    "        #test, question, tag, _, mask, interaction, _ = input\n",
    "        #custum\n",
    "        test, question, tag, _, mask, elapsed, KnowledgeTag_percent, cumulative, paper_number, interaction, _ = input\n",
    "\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "        seq_len = interaction.size(1)\n",
    "\n",
    "        # ì‹ ë‚˜ëŠ” embedding\n",
    "        # ENCODER\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        # print(\"embed_tag size:\", embed_tag.size())\n",
    "        # original\n",
    "        # embed_enc = torch.cat([embed_test,\n",
    "        #                        embed_question,\n",
    "        #                        embed_tag,], 2)\n",
    "        # embed_enc = self.enc_comb_proj(embed_enc)\n",
    "        \n",
    "        #custum\n",
    "        \n",
    "        # print(\"embed_test size:\", embed_test.size())\n",
    "        # print(\"embed_question size:\", embed_question.size())\n",
    "        # print(\"embed_tag size:\", embed_tag.size())\n",
    "        \n",
    "        cate_embed_enc = torch.cat([embed_test,\n",
    "                               embed_question,\n",
    "                               embed_tag,], 2)\n",
    "    \n",
    "        cate_embed_enc = self.enc_cate_comb_proj(cate_embed_enc)\n",
    "        # Positional encoding\n",
    "        # cate_embed_enc = self.pos_encoder(cate_embed_enc)\n",
    "        \n",
    "        cont_embed_enc = torch.cat([#elapsed,\n",
    "                               KnowledgeTag_percent,\n",
    "                               cumulative,], 1)\n",
    "        \n",
    "        cont_embed_enc = cont_embed_enc.view(batch_size, seq_len, -1) # (batch_size , seq_len, cont_col.size())\n",
    "        # cont_embed_enc = self.cont_bn(cont_embed_enc.view(-1, cont_embed_enc.size(-1))) # batchnorm 1d\n",
    "        # cont_embed_enc = cont_embed_enc.view(batch_size, -1, cont_embed_enc.size(-1)) # ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ(batch_size , seq_len, cont_col.size())\n",
    "        # print(cont_embed_enc.size())\n",
    "        cont_embed_enc = self.enc_cont_comb_proj(cont_embed_enc)\n",
    "\n",
    "        seq_emb_enc = torch.cat([cate_embed_enc,cont_embed_enc],2)\n",
    "        # print(\"cont_embed_enc\",cont_embed_enc)\n",
    "#########################################################################################\n",
    "        # DECODER\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "       \n",
    "        # cate data\n",
    "        cate_embed_dec = torch.cat([embed_test,\n",
    "                               embed_question,\n",
    "                               embed_tag,\n",
    "                               embed_interaction], 2)\n",
    "\n",
    "        cate_embed_dec = self.dec_cate_comb_proj(cate_embed_dec)\n",
    "        # Positional encoding\n",
    "        # cate_embed_dec = self.pos_decoder(cate_embed_dec)\n",
    "\n",
    "        # cont data\n",
    "        cont_embed_dec = torch.cat([#elapsed,\n",
    "                               KnowledgeTag_percent,\n",
    "                               cumulative,], 1)\n",
    "        \n",
    "        cont_embed_dec = cont_embed_dec.view(batch_size, seq_len, -1)\n",
    "        # cont_embed_dec = self.cont_bn(cont_embed_dec.view(-1, cont_embed_dec.size(-1))) # batchnorm 1d\n",
    "        # cont_embed_dec = cont_embed_dec.view(batch_size, -1, cont_embed_dec.size(-1)) # ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ(batch_size , seq_len, cont_col.size())\n",
    "        # print(cont_embed_dec.size())\n",
    "        cont_embed_dec = self.dec_cont_comb_proj(cont_embed_dec)\n",
    "        # print(\"cont_embed_dec:\",cont_embed_dec)\n",
    "        seq_emb_dec = torch.cat([cate_embed_dec, cont_embed_dec],2)\n",
    "\n",
    "        # ATTENTION MASK ìƒì„±\n",
    "        # encoderí•˜ê³  decoderì˜ maskëŠ” ê°€ë¡œ ì„¸ë¡œ ê¸¸ì´ê°€ ëª¨ë‘ ë™ì¼í•˜ì—¬\n",
    "        # ì‚¬ì‹¤ ì´ë ‡ê²Œ 3ê°œë¡œ ë‚˜ëˆŒ í•„ìš”ê°€ ì—†ë‹¤\n",
    "        if self.enc_mask is None or self.enc_mask.size(0) != seq_len:\n",
    "            self.enc_mask = self.get_mask(seq_len).to(self.device).to(torch.float32)\n",
    "\n",
    "        if self.dec_mask is None or self.dec_mask.size(0) != seq_len:\n",
    "            self.dec_mask = self.get_mask(seq_len).to(self.device).to(torch.float32)\n",
    "\n",
    "        if self.enc_dec_mask is None or self.enc_dec_mask.size(0) != seq_len:\n",
    "            self.enc_dec_mask = self.get_mask(seq_len).to(self.device).to(torch.float32)\n",
    "\n",
    "        #original\n",
    "        # embed_enc = embed_enc.permute(1, 0, 2)\n",
    "        # embed_dec = embed_dec.permute(1, 0, 2)\n",
    "\n",
    "        # Positional encoding\n",
    "        # embed_enc = self.pos_encoder(embed_enc)\n",
    "        # embed_dec = self.pos_decoder(embed_dec)\n",
    "\n",
    "        # out = self.transformer(embed_enc, embed_dec,\n",
    "        #                        src_mask=self.enc_mask,\n",
    "        #                        tgt_mask=self.dec_mask,\n",
    "        #                        memory_mask=self.enc_dec_mask)\n",
    "        #custum\n",
    "        seq_emb_enc = seq_emb_enc.permute(1, 0, 2)\n",
    "        seq_emb_dec = seq_emb_dec.permute(1, 0, 2)\n",
    "\n",
    "        # Positional encoding custum\n",
    "        seq_emb_enc = self.pos_encoder(seq_emb_enc)\n",
    "        seq_emb_dec = self.pos_decoder(seq_emb_dec)\n",
    "\n",
    "        # print(\"seq_emb_enc:\",seq_emb_enc)\n",
    "        # print(\"seq_emb_dec:\",seq_emb_dec)\n",
    "        # print(\"self.enc_mask:\",self.enc_mask)\n",
    "        # print(\"seq_emb_enc shape:\",seq_emb_enc.shape)\n",
    "        # print(\"seq_emb_dec shape:\",seq_emb_dec.shape)\n",
    "        # print(\"self.enc_dec_mask:\",self.enc_dec_mask)\n",
    "        # std = StandardScaler()\n",
    "        # std.fit(seq_emb_enc)\n",
    "        # seq_emb_enc = std.transform(seq_emb_enc).to(torch.float32).to(self.args.device)\n",
    "\n",
    "        # std = StandardScaler()\n",
    "        # std.fit(seq_emb_dec)\n",
    "        # seq_emb_dec = std.transform(seq_emb_dec).to(torch.float32).to(self.args.device)\n",
    "\n",
    "        # nan ê°’ ì²´í¬\n",
    "        # nan_mask = torch.isnan(seq_emb_enc)\n",
    "        # nan_count = torch.sum(nan_mask).item()\n",
    "        # print(\"seq_emb_enc nan?\",nan_count)\n",
    "        # print(\"seq_emb_enc shape\",seq_emb_enc.shape)\n",
    "        # print(seq_emb_enc)\n",
    "        # nan_mask = torch.isnan(seq_emb_dec)\n",
    "        # nan_count = torch.sum(nan_mask).item()\n",
    "        # print(\"dec_emb_enc nan?\",nan_count)\n",
    "\n",
    "        out = self.transformer(seq_emb_enc, seq_emb_dec,\n",
    "                               src_mask=self.enc_mask,\n",
    "                               tgt_mask=self.dec_mask,\n",
    "                               memory_mask=self.enc_dec_mask)\n",
    "        ###################################################################\n",
    "        # print(\"transformer output:\",out)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, args):\n",
    "    if args.optimizer == 'adam':\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "    if args.optimizer == 'adamW':\n",
    "        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "\n",
    "    # ëª¨ë“  parameterë“¤ì˜ gradê°’ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, args):\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5, mode='max', verbose=True)\n",
    "    elif args.scheduler == 'linear_warmup':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=args.warmup_steps,\n",
    "                                                    num_training_steps=args.total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(pred, target):\n",
    "    loss = nn.BCELoss(reduction=\"none\")\n",
    "    return loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    return auc, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load model and move tensors to a given devices.\n",
    "    \"\"\"\n",
    "    if args.model == 'lstm': model = LSTM(args)\n",
    "    if args.model == 'bert': model = Bert(args)\n",
    "    if args.model == 'last_query': model = LastQuery(args)\n",
    "    if args.model == 'saint': model = Saint(args)\n",
    "    if args.model == 'tfixup': model = FixupEncoder(args)\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ì „ì²˜ë¦¬\n",
    "def process_batch(batch, args):\n",
    "\n",
    "    test, question, tag, correct, mask, elapsed, KnowledgeTag_percent, cumulative, paper_number = batch\n",
    "    # print(\"batch_test_shape : \", test.shape)\n",
    "    # print(\"batch_question_shape : \", question.shape)\n",
    "    # print(\"batch_tag_shape : \", tag.shape)\n",
    "    # print(\"batch_correct_shape : \", correct.shape)\n",
    "    # print(\"batch_mask_shape : \", mask.shape)\n",
    " \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    #  interactionì„ ì„ì‹œì ìœ¼ë¡œ correctë¥¼ í•œì¹¸ ìš°ì¸¡ìœ¼ë¡œ ì´ë™í•œ ê²ƒìœ¼ë¡œ ì‚¬ìš©\n",
    "    #  saintì˜ ê²½ìš° decoderì— ë“¤ì–´ê°€ëŠ” inputì´ë‹¤\n",
    "    interaction = correct + 1 # íŒ¨ë”©ì„ ìœ„í•´ correctê°’ì— 1ì„ ë”í•´ì¤€ë‹¤.\n",
    "    interaction = interaction.roll(shifts=1, dims=1) #dim 1ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ 1ì”© ì´ë™\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "\n",
    "\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # ë§ˆì§€ë§‰ sequenceë§Œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    # device memoryë¡œ ì´ë™\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "\n",
    "\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "\n",
    "    interaction = interaction.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    ############custum##############\n",
    "    #cont ì¶”ê°€\n",
    "    elapsed = elapsed.to(torch.float32)\n",
    "    KnowledgeTag_percent = KnowledgeTag_percent.to(torch.float32)\n",
    "    cumulative = cumulative.to(torch.float32)\n",
    "    paper_number = paper_number.to(torch.float32)\n",
    "\n",
    "    elapsed = elapsed.to(args.device)\n",
    "    KnowledgeTag_percent = KnowledgeTag_percent.to(args.device)\n",
    "    cumulative = cumulative.to(args.device)\n",
    "    paper_number = paper_number.to(args.device)\n",
    "\n",
    "    #original\n",
    "    # return (test, question,\n",
    "    #         tag, correct, mask,\n",
    "    #         interaction, gather_index)\n",
    "\n",
    "    #custum\n",
    "    return (test, question,\n",
    "            tag, correct, mask, \n",
    "            elapsed, KnowledgeTag_percent, cumulative, paper_number,\n",
    "            interaction, gather_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossê³„ì‚°í•˜ê³  parameter update!\n",
    "def compute_loss(preds, targets, index):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "        index    : (batch_size, max_seq_len)\n",
    "\n",
    "        ë§Œì•½ ì „ì²´ sequence ê¸¸ì´ê°€ max_seq_lenë³´ë‹¤ ì‘ë‹¤ë©´ í•´ë‹¹ ê¸¸ì´ë¡œ ì§„í–‰\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    loss = torch.gather(loss, 1, index)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(model):\n",
    "    gradient = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        grad = param.grad\n",
    "        if grad != None:\n",
    "            gradient.append(grad.cpu().numpy().astype(np.float16))\n",
    "            # gradient.append(grad.clone().detach())\n",
    "        else:\n",
    "            gradient.append(None)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, args, gradient=False):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        \n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "        index = input[-1] # gather index\n",
    "        # print(\"preds shape\",preds)\n",
    "        # print(\"targets shape\",targets.dtype)\n",
    "        # print(\"index shape\",index.dtype)\n",
    "        loss = compute_loss(preds, targets, index)\n",
    "        loss.backward()\n",
    "\n",
    "        # save gradient distribution\n",
    "        if gradient:\n",
    "            args.n_iteration += 1\n",
    "            args.gradient[f'iteration_{args.n_iteration}'] = get_gradient(model)\n",
    "\n",
    "        # grad clip\n",
    "        if args.clip_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # warmup scheduler\n",
    "        if args.scheduler == 'linear_warmup':\n",
    "            scheduler.step()\n",
    "\n",
    "        # predictions\n",
    "        preds = preds.gather(1, index).view(-1)\n",
    "        targets = targets.gather(1, index).view(-1)\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "\n",
    "    return auc, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "        index = input[-1] # gather index\n",
    "\n",
    "        # predictions\n",
    "        preds = preds.gather(1, index).view(-1)\n",
    "        targets = targets.gather(1, index).view(-1)\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "\n",
    "    return auc, acc, total_preds, total_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args, train_data, valid_data, gradient=False):\n",
    "\n",
    "    # ìºì‹œ ë©”ëª¨ë¦¬ ë¹„ìš°ê¸° ë° ê°€ë¹„ì§€ ì»¬ë ‰í„° ê°€ë™!\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # augmentation\n",
    "    augmented_train_data = data_augmentation(train_data, args)\n",
    "    if len(augmented_train_data) != len(train_data):\n",
    "        print(f\"Data Augmentation applied. Train data {len(train_data)} -> {len(augmented_train_data)}\\n\")\n",
    "\n",
    "    train_loader, valid_loader = get_loaders(args, augmented_train_data, valid_data)\n",
    "\n",
    "    # only when using warmup scheduler\n",
    "    args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "    args.warmup_steps = args.total_steps // 10\n",
    "\n",
    "    model = get_model(args)\n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "    # ğŸŒŸ ë¶„ì„ì— ì‚¬ìš©í•  ê°’ ì €ì¥ ğŸŒŸ\n",
    "    report = OrderedDict()\n",
    "\n",
    "    # gradient step ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜\n",
    "    if gradient:\n",
    "        args.n_iteration = 0\n",
    "        args.gradient = OrderedDict()\n",
    "\n",
    "        # ëª¨ë¸ì˜ gradientê°’ì„ ê°€ë¦¬í‚¤ëŠ” ëª¨ë¸ ëª… ì €ì¥\n",
    "        args.gradient['name'] = [name for name, _ in model.named_parameters()]\n",
    "\n",
    "    best_auc = -1\n",
    "    best_auc_epoch = -1\n",
    "    best_acc = -1\n",
    "    best_acc_epoch = -1\n",
    "    for epoch in notebook.tqdm(range(args.n_epochs)):\n",
    "        epoch_report = {}\n",
    "\n",
    "        ### TRAIN\n",
    "        train_start_time = time.time()\n",
    "        train_auc, train_acc = train(train_loader, model, optimizer, scheduler, args, gradient)\n",
    "        train_time = time.time() - train_start_time\n",
    "\n",
    "        epoch_report['train_auc'] = train_auc\n",
    "        epoch_report['train_acc'] = train_acc\n",
    "        epoch_report['train_time'] = train_time\n",
    "\n",
    "        ### VALID\n",
    "        valid_start_time = time.time()\n",
    "        valid_auc, valid_acc, preds, targets = validate(valid_loader, model, args)\n",
    "        valid_time = time.time() - valid_start_time\n",
    "\n",
    "        epoch_report['valid_auc'] = valid_auc\n",
    "        epoch_report['valid_acc'] = valid_acc\n",
    "        epoch_report['valid_time'] = valid_time\n",
    "\n",
    "        # save lr\n",
    "        epoch_report['lr'] = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "        # ğŸŒŸ save it to report ğŸŒŸ\n",
    "        report[f'{epoch + 1}'] = epoch_report\n",
    "\n",
    "\n",
    "        ### TODO: model save or early stopping\n",
    "        if valid_auc > best_auc:\n",
    "            best_auc = valid_auc\n",
    "            best_auc_epoch = epoch + 1\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_acc_epoch = epoch + 1\n",
    "\n",
    "        # scheduler\n",
    "        if args.scheduler == 'plateau':\n",
    "            scheduler.step(best_auc)\n",
    "\n",
    "    # save best records\n",
    "    report['best_auc'] = best_auc\n",
    "    report['best_auc_epoch'] = best_auc_epoch\n",
    "    report['best_acc'] = best_acc\n",
    "    report['best_acc_epoch'] = best_acc_epoch\n",
    "\n",
    "    # save gradient informations\n",
    "    if gradient:\n",
    "        report['gradient'] = args.gradient\n",
    "        del args.gradient\n",
    "        del args['gradient']\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "DATA_PATH = './data'\n",
    "FILE_PATH = 'train_data.csv'\n",
    "# ì„¤ì •\n",
    "config['seed'] = 42\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['data_dir'] = DATA_PATH\n",
    "\n",
    "# ë°ì´í„°\n",
    "config['max_seq_len'] = 300\n",
    "\n",
    "# ë°ì´í„° ì¦ê°• (Data Augmentation)\n",
    "config['window'] = False\n",
    "config['stride'] = config['max_seq_len']\n",
    "config['shuffle'] = False\n",
    "config['shuffle_n'] = 2\n",
    "\n",
    "# ëª¨ë¸\n",
    "config['hidden_dim'] = 128\n",
    "config['n_layers'] = 1\n",
    "config['dropout'] = 0.0\n",
    "config['n_heads'] = 4\n",
    "\n",
    "# T Fixup\n",
    "config['Tfixup'] = False\n",
    "config['layer_norm'] = True\n",
    "\n",
    "# í›ˆë ¨\n",
    "config['n_epochs'] = 10\n",
    "config['batch_size'] = 64\n",
    "config['lr'] = 0.0001\n",
    "config['clip_grad'] = 2.0\n",
    "\n",
    "### ì¤‘ìš” ###\n",
    "config['model'] = 'saint'\n",
    "config['optimizer'] = 'adam'\n",
    "config['scheduler'] = 'plateau'\n",
    "\n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed nanê°’ì˜ ê°œìˆ˜: 0\n",
      "KnowledgeTag_percent nanê°’ì˜ ê°œìˆ˜: 0\n",
      "cumulative nanê°’ì˜ ê°œìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(FILE_PATH)\n",
    "\n",
    "train_data = preprocess.get_train_data()\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨(train) ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ : 4688 ê°œ\n",
      "ê²€ì¦(valid) ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ : 2010 ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(f\"í›ˆë ¨(train) ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ : {len(train_data)} ê°œ\")\n",
    "print(f\"ê²€ì¦(valid) ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ : {len(valid_data)} ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testID len : 24\n",
      "assessmentItemID len : 24\n",
      "KnowledgeTag size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "testID len : 573\n",
      "assessmentItemID len : 573\n",
      "KnowledgeTag size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "testID len : 74\n",
      "assessmentItemID len : 74\n",
      "KnowledgeTag size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "testID len : 110\n",
      "assessmentItemID len : 110\n",
      "KnowledgeTag size : 110\n",
      "target size : 110\n",
      "target size : 110\n",
      "target size : 110\n",
      "target size : 110\n",
      "target size : 110\n"
     ]
    }
   ],
   "source": [
    "# ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì£¼ì–´ì§€ëŠ” ë°ì´í„°ë¥¼ ì‚´í´ë³´ì\n",
    "for i,group in enumerate(np.asarray(train_data)):\n",
    "     a,b,c,d,e,f,g,h = np.asarray(group)\n",
    "     print(f\"testID len : {len(a)}\")\n",
    "     print(f\"assessmentItemID len : {len(b)}\")\n",
    "     print(f\"KnowledgeTag size : {len(c)}\")\n",
    "     print(f\"target size : {len(d)}\")\n",
    "     print(f\"target size : {len(e)}\")\n",
    "     print(f\"target size : {len(f)}\")\n",
    "     print(f\"target size : {len(g)}\")\n",
    "     print(f\"target size : {len(h)}\")\n",
    "     if i ==3:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œì²˜ : https://www.kaggle.com/bminixhofer/a-validation-framework-impact-of-the-random-seed\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reportì—ì„œ aucë° ì‹¤í–‰ ì‹œê°„ ì •ë³´ ì–»ê¸°\n",
    "def time_auc(report, n_epoch=10):\n",
    "    total_time = 0\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        result = report[str(epoch)]\n",
    "        total_time += result['train_time']\n",
    "        total_time += result['valid_time']\n",
    "\n",
    "    return total_time, report['best_auc'], report['best_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac005c980590475da55570898007ca33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost Time : 134.3642065525055 sec, best AUC : 0.7089434184204289\n"
     ]
    }
   ],
   "source": [
    "# seed ì„¤ì •\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Gradient ë¶„í¬ë„ ì²´í¬í•  ê²ƒì´ë¯€ë¡œ Trueë¡œ í‘œì‹œ\n",
    "report = run(args, train_data, valid_data, gradient=True)\n",
    "total_time, auc, acc = time_auc(report)\n",
    "\n",
    "print(f\"Cost Time : {total_time} sec, best AUC : {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC : 0.7089434184204289 at epoch 9\n",
      "ACC : 0.6651741293532338 at epoch 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC : {report['best_auc']} at epoch {report['best_auc_epoch']}\")\n",
    "print(f\"ACC : {report['best_acc']} at epoch {report['best_acc_epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
