{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 사용 가능 -> True, GPU 사용 불가 -> False\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import random\n",
    "import easydict\n",
    "import tarfile\n",
    "\n",
    "from tqdm import notebook\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed(df) :\n",
    "    diff_train = df.loc[:, ['userID','Timestamp']].groupby('userID').diff().shift(-1)\n",
    "    diff_train = diff_train['Timestamp'].apply(lambda x : x.total_seconds())\n",
    "    df['elapsed'] = diff_train\n",
    "    \n",
    "    df.groupby('userID').apply(lambda x :x.iloc[:-1])\n",
    "\n",
    "    # 한 시간이 지나면 outlier로 처리\n",
    "    outlier = 1*3600\n",
    "    non_outlier = df[df['elapsed'] <= outlier]\n",
    "    # outlier에 해당하지 않는 row로 재구성 한 후 각 태그의 평균처리\n",
    "    mean_elapsed = non_outlier.groupby('KnowledgeTag')['elapsed'].mean()\n",
    "    df.loc[df['elapsed'] > outlier, 'elapsed'] = df[df['elapsed'] > outlier].apply(lambda x: mean_elapsed.get(x['KnowledgeTag'], x['elapsed']), axis=1)\n",
    "    df['elapsed'] = df['elapsed'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def cumsum(df) :\n",
    "    # 누적합\n",
    "    _cumsum = df.loc[:, ['userID', 'answerCode']].groupby('userID').agg({'answerCode': 'cumsum'})\n",
    "    # 누적갯수\n",
    "    _cumcount = df.loc[:, ['userID', 'answerCode']].groupby('userID').agg({'answerCode': 'cumcount'}) + 1\n",
    "\n",
    "    cum_ans = _cumsum / _cumcount\n",
    "    df['cumulative'] = cum_ans['answerCode']\n",
    "\n",
    "    df['paper_number'] = df['assessmentItemID'].apply(lambda x: x[7:]) # assessmentItemID의 뒤에 3자리를 의미 -> 각 시험지 별로 문제번호\n",
    "    # item 열을 int16으로 변경\n",
    "    df[\"paper_number\"] = df[\"paper_number\"].astype(\"int16\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def avg_percent(x) :\n",
    "    return np.sum(x) / len(x)\n",
    "\n",
    "def type_percent(df) :\n",
    "    # 위에서 처리한 type을 변환하여 각각의 정답률 처리\n",
    "\n",
    "    df['KnowledgeTag_percent'] = df.groupby('KnowledgeTag')['answerCode'].transform(avg_percent)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.data_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df):\n",
    "        #con_col에 대한 전처리\n",
    "        ###TODO: con_col에 대한 전처리 코드 추가\n",
    "        con_cols= [\"elapsed\", \"KnowledgeTag_percent\", \"cumulative\", \"paper_number\"]\n",
    "        df = elapsed(df)\n",
    "        df = cumsum(df)\n",
    "        df = type_percent(df)\n",
    "        #################################CUSTUM#############################################\n",
    "        \n",
    "        #cate_col에 대한 전처리\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "        for col in cate_cols:\n",
    "\n",
    "            #For UNKNOWN class\n",
    "            #마지막을 nan값으로 준 이유는 마스킹 때문이라고 생각\n",
    "            a = df[col].unique().tolist() + [np.nan]\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(a)\n",
    "            df[col] = le.transform(df[col])\n",
    "            self.__save_labels(le, col)\n",
    "\n",
    "        # def convert_time(s):\n",
    "        #     timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "        #     return int(timestamp)\n",
    "\n",
    "        # df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name):\n",
    "       \n",
    "        #################custun#########################\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        ######################################################\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path,dtype=dtype, parse_dates=['Timestamp'])\n",
    "        df = self.__preprocessing(df)\n",
    "        print(\"elapsed nan값의 개수:\",df[\"elapsed\"].isna().sum())\n",
    "        print(\"KnowledgeTag_percent nan값의 개수:\",df[\"KnowledgeTag_percent\"].isna().sum())\n",
    "        print(\"cumulative nan값의 개수:\",df[\"cumulative\"].isna().sum())\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "        self.args.n_questions = df['assessmentItemID'].nunique()\n",
    "        self.args.n_test = df['testId'].nunique()\n",
    "        self.args.n_tag = df['KnowledgeTag'].nunique() \n",
    "        \n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "        #기존 columns\n",
    "        #columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag']\n",
    "        # group = df[columns].groupby('userID').apply(\n",
    "        #     lambda r: (\n",
    "        #         r['testId'].values,\n",
    "        #         r['assessmentItemID'].values,\n",
    "        #         r['KnowledgeTag'].values,\n",
    "        #         r['answerCode'].values\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        #columns 추가\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag',\"elapsed\", \"KnowledgeTag_percent\", \"cumulative\", \"paper_number\"]\n",
    "\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values,\n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values,\n",
    "                    r['elapsed'].values,\n",
    "                    r['KnowledgeTag_percent'].values,\n",
    "                    r['cumulative'].values,\n",
    "                    r['paper_number'].values,\n",
    "                )\n",
    "            )\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "        \n",
    "        #original\n",
    "        #test, question, tag, correct = row[0], row[1], row[2], row[3]\n",
    "        #custum\n",
    "        test, question, tag, correct, elapsed, KnowledgeTag_percent, cumulative, paper_number = row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]\n",
    "\n",
    "        cate_cols = [test, question, tag, correct]\n",
    "        \n",
    "        #custum\n",
    "        cont_cols = [elapsed, KnowledgeTag_percent, cumulative, paper_number]\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            # cate_col\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "            # cont_col\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[:seq_len] = 1\n",
    "        \n",
    "        \n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        # if seq_len > self.args.max_seq_len:\n",
    "        #     for i, col in enumerate(cate_cols):\n",
    "        #         cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "        #     mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        # else:\n",
    "        #     mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "        #     mask[:seq_len] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        #custum\n",
    "        cate_cont_cols = []\n",
    "        cate_cont_cols.extend(cate_cols)\n",
    "        cate_cont_cols.extend(cont_cols)\n",
    "        ############################################\n",
    "        #original\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        # for i, col in enumerate(cate_cols):\n",
    "        #     cate_cols[i] = torch.tensor(col)\n",
    "        # return cate_cols\n",
    "        #custum\n",
    "        for i, col in enumerate(cate_cont_cols):\n",
    "            cate_cont_cols[i] = torch.tensor(col)\n",
    "        return cate_cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#padding을 위한 함수\n",
    "def collate(batch):\n",
    "    col_n = len(batch[0])\n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    # print(\"column 개수\",col_n)\n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row):\n",
    "            col_list[i].append(col)\n",
    "\n",
    "    # 각 column의 값들을 대상으로 padding 진행\n",
    "    # pad_sequence([[1, 2, 3], [3, 4]]) -> [[1, 2, 3],\n",
    "    #                                       [3, 4, 0]]\n",
    "    for i, col_batch in enumerate(col_list):\n",
    "        col_list[i] = pad_sequence(col_batch, batch_first=True)\n",
    "\n",
    "    # mask의 경우 max_seq_len을 기준으로 길이가 설정되어있다.\n",
    "    # 만약 다른 column들의 seq_len이 max_seq_len보다 작다면\n",
    "    # 이 길이에 맞추어 mask의 길이도 조절해준다\n",
    "    col_seq_len = col_list[0].size(1)\n",
    "    mask_seq_len = col_list[-1].size(1)\n",
    "    if col_seq_len < mask_seq_len:\n",
    "        col_list[-1] = col_list[-1][:, :col_seq_len]\n",
    "\n",
    "    return tuple(col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "\n",
    "    trainset = DKTDataset(train, args)\n",
    "    valset = DKTDataset(valid, args)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, shuffle=True,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               pin_memory=pin_memory,\n",
    "                                               collate_fn=collate)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(valset, shuffle=False,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               pin_memory=pin_memory,\n",
    "                                               collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slidding_window(data, args):\n",
    "    window_size = args.max_seq_len\n",
    "    stride = args.stride\n",
    "\n",
    "    augmented_datas = []\n",
    "    for row in data:\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        # 만약 window 크기보다 seq len이 같거나 작으면 augmentation을 하지 않는다\n",
    "        if seq_len <= window_size:\n",
    "            augmented_datas.append(row)\n",
    "        else:\n",
    "            total_window = ((seq_len - window_size) // stride) + 1\n",
    "\n",
    "            # 앞에서부터 slidding window 적용\n",
    "            for window_i in range(total_window):\n",
    "                # window로 잘린 데이터를 모으는 리스트\n",
    "                window_data = []\n",
    "                for col in row:\n",
    "                    window_data.append(col[window_i*stride:window_i*stride + window_size])\n",
    "\n",
    "                # Shuffle\n",
    "                # 마지막 데이터의 경우 shuffle을 하지 않는다\n",
    "                if args.shuffle and window_i + 1 != total_window:\n",
    "                    shuffle_datas = shuffle(window_data, window_size, args)\n",
    "                    augmented_datas += shuffle_datas\n",
    "                else:\n",
    "                    augmented_datas.append(tuple(window_data))\n",
    "\n",
    "            # slidding window에서 뒷부분이 누락될 경우 추가\n",
    "            total_len = window_size + (stride * (total_window - 1))\n",
    "            if seq_len != total_len:\n",
    "                window_data = []\n",
    "                for col in row:\n",
    "                    window_data.append(col[-window_size:])\n",
    "                augmented_datas.append(tuple(window_data))\n",
    "\n",
    "\n",
    "    return augmented_datas\n",
    "\n",
    "def shuffle(data, data_size, args):\n",
    "    shuffle_datas = []\n",
    "    for i in range(args.shuffle_n):\n",
    "        # shuffle 횟수만큼 window를 랜덤하게 계속 섞어서 데이터로 추가\n",
    "        shuffle_data = []\n",
    "        random_index = np.random.permutation(data_size)\n",
    "        for col in data:\n",
    "            shuffle_data.append(col[random_index])\n",
    "        shuffle_datas.append(tuple(shuffle_data))\n",
    "    return shuffle_datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(data, args):\n",
    "    if args.window == True:\n",
    "        data = slidding_window(data, args)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # input embedding\n",
    "        pe = torch.zeros(max_len, d_model) ## max_len X hidden_dim\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #0부터 sequence 길이만큼 position 값 생성, 1 X max_len\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Saint(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Saint, self).__init__()\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        # self.dropout = self.args.dropout\n",
    "        self.dropout = 0.\n",
    "\n",
    "        ### Embedding\n",
    "        # ENCODER embedding\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # encoder combination projection\n",
    "        # original\n",
    "        # self.enc_comb_proj = nn.Linear((self.hidden_dim//3)*3, self.hidden_dim)\n",
    "        # custum\n",
    "        self.enc_cate_comb_proj = nn.Linear((self.hidden_dim//3)*3, self.hidden_dim//2)\n",
    "        self.enc_cont_comb_proj = nn.Linear(2, self.hidden_dim//2) ## 임시로 현재 3개로 지정, 추후 코드 변경 필요, cont column의 개수임\n",
    "        # batchnorm 추가\n",
    "        # self.cont_bn = nn.BatchNorm1d(3)\n",
    "        # # 재수정\n",
    "        # self.enc_cate_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear((self.hidden_dim//3)*3, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        # self.enc_cont_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear(3, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        ###########################################################################\n",
    "        \n",
    "        # DECODER embedding\n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "\n",
    "        # decoder combination projection\n",
    "        # original\n",
    "        # self.dec_comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "        # custum\n",
    "        self.dec_cate_comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim//2)\n",
    "        self.dec_cont_comb_proj = nn.Linear(2, self.hidden_dim//2)## 임시로 현재 3개로 지정, 추후 코드 변경 필요, cont column의 개수임\n",
    "        # 재수정\n",
    "        # self.dec_cate_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear((self.hidden_dim//3)*4, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        # self.dec_cont_comb_proj = nn.Sequential(nn.ReLU(),\n",
    "        #                                         nn.Linear(3, self.hidden_dim//2),\n",
    "        #                                         nn.LayerNorm(self.hidden_dim//2))\n",
    "        ###########################################################################\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(self.hidden_dim, self.dropout, self.args.max_seq_len)\n",
    "        self.pos_decoder = PositionalEncoding(self.hidden_dim, self.dropout, self.args.max_seq_len)\n",
    "        # cate data에만 positional encoding 적용하는 코드로 임시 수정\n",
    "        # self.pos_encoder = PositionalEncoding(self.hidden_dim//2, self.dropout, self.args.max_seq_len)\n",
    "        # self.pos_decoder = PositionalEncoding(self.hidden_dim//2, self.dropout, self.args.max_seq_len)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=self.args.n_heads,\n",
    "            num_encoder_layers=self.args.n_layers,\n",
    "            num_decoder_layers=self.args.n_layers,\n",
    "            dim_feedforward=self.hidden_dim,\n",
    "            dropout=self.dropout,\n",
    "            activation='relu')\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        self.enc_mask = None\n",
    "        self.dec_mask = None\n",
    "        self.enc_dec_mask = None\n",
    "\n",
    "    def get_mask(self, seq_len):\n",
    "        mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1))\n",
    "\n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #original\n",
    "        #test, question, tag, _, mask, interaction, _ = input\n",
    "        #custum\n",
    "        test, question, tag, _, mask, elapsed, KnowledgeTag_percent, cumulative, paper_number, interaction, _ = input\n",
    "\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "        seq_len = interaction.size(1)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        # ENCODER\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        # print(\"embed_tag size:\", embed_tag.size())\n",
    "        # original\n",
    "        # embed_enc = torch.cat([embed_test,\n",
    "        #                        embed_question,\n",
    "        #                        embed_tag,], 2)\n",
    "        # embed_enc = self.enc_comb_proj(embed_enc)\n",
    "        \n",
    "        #custum\n",
    "        \n",
    "        # print(\"embed_test size:\", embed_test.size())\n",
    "        # print(\"embed_question size:\", embed_question.size())\n",
    "        # print(\"embed_tag size:\", embed_tag.size())\n",
    "        \n",
    "        cate_embed_enc = torch.cat([embed_test,\n",
    "                               embed_question,\n",
    "                               embed_tag,], 2)\n",
    "    \n",
    "        cate_embed_enc = self.enc_cate_comb_proj(cate_embed_enc)\n",
    "        # Positional encoding\n",
    "        # cate_embed_enc = self.pos_encoder(cate_embed_enc)\n",
    "        \n",
    "        cont_embed_enc = torch.cat([#elapsed,\n",
    "                               KnowledgeTag_percent,\n",
    "                               cumulative,], 1)\n",
    "        \n",
    "        cont_embed_enc = cont_embed_enc.view(batch_size, seq_len, -1) # (batch_size , seq_len, cont_col.size())\n",
    "        # cont_embed_enc = self.cont_bn(cont_embed_enc.view(-1, cont_embed_enc.size(-1))) # batchnorm 1d\n",
    "        # cont_embed_enc = cont_embed_enc.view(batch_size, -1, cont_embed_enc.size(-1)) # 다시 원래대로(batch_size , seq_len, cont_col.size())\n",
    "        # print(cont_embed_enc.size())\n",
    "        cont_embed_enc = self.enc_cont_comb_proj(cont_embed_enc)\n",
    "\n",
    "        seq_emb_enc = torch.cat([cate_embed_enc,cont_embed_enc],2)\n",
    "        # print(\"cont_embed_enc\",cont_embed_enc)\n",
    "#########################################################################################\n",
    "        # DECODER\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "       \n",
    "        # cate data\n",
    "        cate_embed_dec = torch.cat([embed_test,\n",
    "                               embed_question,\n",
    "                               embed_tag,\n",
    "                               embed_interaction], 2)\n",
    "\n",
    "        cate_embed_dec = self.dec_cate_comb_proj(cate_embed_dec)\n",
    "        # Positional encoding\n",
    "        # cate_embed_dec = self.pos_decoder(cate_embed_dec)\n",
    "\n",
    "        # cont data\n",
    "        cont_embed_dec = torch.cat([#elapsed,\n",
    "                               KnowledgeTag_percent,\n",
    "                               cumulative,], 1)\n",
    "        \n",
    "        cont_embed_dec = cont_embed_dec.view(batch_size, seq_len, -1)\n",
    "        # cont_embed_dec = self.cont_bn(cont_embed_dec.view(-1, cont_embed_dec.size(-1))) # batchnorm 1d\n",
    "        # cont_embed_dec = cont_embed_dec.view(batch_size, -1, cont_embed_dec.size(-1)) # 다시 원래대로(batch_size , seq_len, cont_col.size())\n",
    "        # print(cont_embed_dec.size())\n",
    "        cont_embed_dec = self.dec_cont_comb_proj(cont_embed_dec)\n",
    "        # print(\"cont_embed_dec:\",cont_embed_dec)\n",
    "        seq_emb_dec = torch.cat([cate_embed_dec, cont_embed_dec],2)\n",
    "\n",
    "        # ATTENTION MASK 생성\n",
    "        # encoder하고 decoder의 mask는 가로 세로 길이가 모두 동일하여\n",
    "        # 사실 이렇게 3개로 나눌 필요가 없다\n",
    "        if self.enc_mask is None or self.enc_mask.size(0) != seq_len:\n",
    "            self.enc_mask = self.get_mask(seq_len).to(self.device).to(torch.float32)\n",
    "\n",
    "        if self.dec_mask is None or self.dec_mask.size(0) != seq_len:\n",
    "            self.dec_mask = self.get_mask(seq_len).to(self.device).to(torch.float32)\n",
    "\n",
    "        if self.enc_dec_mask is None or self.enc_dec_mask.size(0) != seq_len:\n",
    "            self.enc_dec_mask = self.get_mask(seq_len).to(self.device).to(torch.float32)\n",
    "\n",
    "        #original\n",
    "        # embed_enc = embed_enc.permute(1, 0, 2)\n",
    "        # embed_dec = embed_dec.permute(1, 0, 2)\n",
    "\n",
    "        # Positional encoding\n",
    "        # embed_enc = self.pos_encoder(embed_enc)\n",
    "        # embed_dec = self.pos_decoder(embed_dec)\n",
    "\n",
    "        # out = self.transformer(embed_enc, embed_dec,\n",
    "        #                        src_mask=self.enc_mask,\n",
    "        #                        tgt_mask=self.dec_mask,\n",
    "        #                        memory_mask=self.enc_dec_mask)\n",
    "        #custum\n",
    "        seq_emb_enc = seq_emb_enc.permute(1, 0, 2)\n",
    "        seq_emb_dec = seq_emb_dec.permute(1, 0, 2)\n",
    "\n",
    "        # Positional encoding custum\n",
    "        seq_emb_enc = self.pos_encoder(seq_emb_enc)\n",
    "        seq_emb_dec = self.pos_decoder(seq_emb_dec)\n",
    "\n",
    "        # print(\"seq_emb_enc:\",seq_emb_enc)\n",
    "        # print(\"seq_emb_dec:\",seq_emb_dec)\n",
    "        # print(\"self.enc_mask:\",self.enc_mask)\n",
    "        # print(\"seq_emb_enc shape:\",seq_emb_enc.shape)\n",
    "        # print(\"seq_emb_dec shape:\",seq_emb_dec.shape)\n",
    "        # print(\"self.enc_dec_mask:\",self.enc_dec_mask)\n",
    "        # std = StandardScaler()\n",
    "        # std.fit(seq_emb_enc)\n",
    "        # seq_emb_enc = std.transform(seq_emb_enc).to(torch.float32).to(self.args.device)\n",
    "\n",
    "        # std = StandardScaler()\n",
    "        # std.fit(seq_emb_dec)\n",
    "        # seq_emb_dec = std.transform(seq_emb_dec).to(torch.float32).to(self.args.device)\n",
    "\n",
    "        # nan 값 체크\n",
    "        # nan_mask = torch.isnan(seq_emb_enc)\n",
    "        # nan_count = torch.sum(nan_mask).item()\n",
    "        # print(\"seq_emb_enc nan?\",nan_count)\n",
    "        # print(\"seq_emb_enc shape\",seq_emb_enc.shape)\n",
    "        # print(seq_emb_enc)\n",
    "        # nan_mask = torch.isnan(seq_emb_dec)\n",
    "        # nan_count = torch.sum(nan_mask).item()\n",
    "        # print(\"dec_emb_enc nan?\",nan_count)\n",
    "\n",
    "        out = self.transformer(seq_emb_enc, seq_emb_dec,\n",
    "                               src_mask=self.enc_mask,\n",
    "                               tgt_mask=self.dec_mask,\n",
    "                               memory_mask=self.enc_dec_mask)\n",
    "        ###################################################################\n",
    "        # print(\"transformer output:\",out)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, args):\n",
    "    if args.optimizer == 'adam':\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "    if args.optimizer == 'adamW':\n",
    "        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "\n",
    "    # 모든 parameter들의 grad값을 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, args):\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5, mode='max', verbose=True)\n",
    "    elif args.scheduler == 'linear_warmup':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=args.warmup_steps,\n",
    "                                                    num_training_steps=args.total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(pred, target):\n",
    "    loss = nn.BCELoss(reduction=\"none\")\n",
    "    return loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    return auc, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load model and move tensors to a given devices.\n",
    "    \"\"\"\n",
    "    if args.model == 'lstm': model = LSTM(args)\n",
    "    if args.model == 'bert': model = Bert(args)\n",
    "    if args.model == 'last_query': model = LastQuery(args)\n",
    "    if args.model == 'saint': model = Saint(args)\n",
    "    if args.model == 'tfixup': model = FixupEncoder(args)\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 전처리\n",
    "def process_batch(batch, args):\n",
    "\n",
    "    test, question, tag, correct, mask, elapsed, KnowledgeTag_percent, cumulative, paper_number = batch\n",
    "    # print(\"batch_test_shape : \", test.shape)\n",
    "    # print(\"batch_question_shape : \", question.shape)\n",
    "    # print(\"batch_tag_shape : \", tag.shape)\n",
    "    # print(\"batch_correct_shape : \", correct.shape)\n",
    "    # print(\"batch_mask_shape : \", mask.shape)\n",
    " \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    #  interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    #  saint의 경우 decoder에 들어가는 input이다\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1) #dim 1에 해당하는 값을 1씩 이동\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "\n",
    "\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # 마지막 sequence만 사용하기 위한 index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    # device memory로 이동\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "\n",
    "\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "\n",
    "    interaction = interaction.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    ############custum##############\n",
    "    #cont 추가\n",
    "    elapsed = elapsed.to(torch.float32)\n",
    "    KnowledgeTag_percent = KnowledgeTag_percent.to(torch.float32)\n",
    "    cumulative = cumulative.to(torch.float32)\n",
    "    paper_number = paper_number.to(torch.float32)\n",
    "\n",
    "    elapsed = elapsed.to(args.device)\n",
    "    KnowledgeTag_percent = KnowledgeTag_percent.to(args.device)\n",
    "    cumulative = cumulative.to(args.device)\n",
    "    paper_number = paper_number.to(args.device)\n",
    "\n",
    "    #original\n",
    "    # return (test, question,\n",
    "    #         tag, correct, mask,\n",
    "    #         interaction, gather_index)\n",
    "\n",
    "    #custum\n",
    "    return (test, question,\n",
    "            tag, correct, mask, \n",
    "            elapsed, KnowledgeTag_percent, cumulative, paper_number,\n",
    "            interaction, gather_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss계산하고 parameter update!\n",
    "def compute_loss(preds, targets, index):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "        index    : (batch_size, max_seq_len)\n",
    "\n",
    "        만약 전체 sequence 길이가 max_seq_len보다 작다면 해당 길이로 진행\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    loss = torch.gather(loss, 1, index)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(model):\n",
    "    gradient = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        grad = param.grad\n",
    "        if grad != None:\n",
    "            gradient.append(grad.cpu().numpy().astype(np.float16))\n",
    "            # gradient.append(grad.clone().detach())\n",
    "        else:\n",
    "            gradient.append(None)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, args, gradient=False):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        \n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "        index = input[-1] # gather index\n",
    "        # print(\"preds shape\",preds)\n",
    "        # print(\"targets shape\",targets.dtype)\n",
    "        # print(\"index shape\",index.dtype)\n",
    "        loss = compute_loss(preds, targets, index)\n",
    "        loss.backward()\n",
    "\n",
    "        # save gradient distribution\n",
    "        if gradient:\n",
    "            args.n_iteration += 1\n",
    "            args.gradient[f'iteration_{args.n_iteration}'] = get_gradient(model)\n",
    "\n",
    "        # grad clip\n",
    "        if args.clip_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # warmup scheduler\n",
    "        if args.scheduler == 'linear_warmup':\n",
    "            scheduler.step()\n",
    "\n",
    "        # predictions\n",
    "        preds = preds.gather(1, index).view(-1)\n",
    "        targets = targets.gather(1, index).view(-1)\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "\n",
    "    return auc, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "        index = input[-1] # gather index\n",
    "\n",
    "        # predictions\n",
    "        preds = preds.gather(1, index).view(-1)\n",
    "        targets = targets.gather(1, index).view(-1)\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "\n",
    "    return auc, acc, total_preds, total_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args, train_data, valid_data, gradient=False):\n",
    "\n",
    "    # 캐시 메모리 비우기 및 가비지 컬렉터 가동!\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # augmentation\n",
    "    augmented_train_data = data_augmentation(train_data, args)\n",
    "    if len(augmented_train_data) != len(train_data):\n",
    "        print(f\"Data Augmentation applied. Train data {len(train_data)} -> {len(augmented_train_data)}\\n\")\n",
    "\n",
    "    train_loader, valid_loader = get_loaders(args, augmented_train_data, valid_data)\n",
    "\n",
    "    # only when using warmup scheduler\n",
    "    args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "    args.warmup_steps = args.total_steps // 10\n",
    "\n",
    "    model = get_model(args)\n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "    # 🌟 분석에 사용할 값 저장 🌟\n",
    "    report = OrderedDict()\n",
    "\n",
    "    # gradient step 분석에 사용할 변수\n",
    "    if gradient:\n",
    "        args.n_iteration = 0\n",
    "        args.gradient = OrderedDict()\n",
    "\n",
    "        # 모델의 gradient값을 가리키는 모델 명 저장\n",
    "        args.gradient['name'] = [name for name, _ in model.named_parameters()]\n",
    "\n",
    "    best_auc = -1\n",
    "    best_auc_epoch = -1\n",
    "    best_acc = -1\n",
    "    best_acc_epoch = -1\n",
    "    for epoch in notebook.tqdm(range(args.n_epochs)):\n",
    "        epoch_report = {}\n",
    "\n",
    "        ### TRAIN\n",
    "        train_start_time = time.time()\n",
    "        train_auc, train_acc = train(train_loader, model, optimizer, scheduler, args, gradient)\n",
    "        train_time = time.time() - train_start_time\n",
    "\n",
    "        epoch_report['train_auc'] = train_auc\n",
    "        epoch_report['train_acc'] = train_acc\n",
    "        epoch_report['train_time'] = train_time\n",
    "\n",
    "        ### VALID\n",
    "        valid_start_time = time.time()\n",
    "        valid_auc, valid_acc, preds, targets = validate(valid_loader, model, args)\n",
    "        valid_time = time.time() - valid_start_time\n",
    "\n",
    "        epoch_report['valid_auc'] = valid_auc\n",
    "        epoch_report['valid_acc'] = valid_acc\n",
    "        epoch_report['valid_time'] = valid_time\n",
    "\n",
    "        # save lr\n",
    "        epoch_report['lr'] = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "        # 🌟 save it to report 🌟\n",
    "        report[f'{epoch + 1}'] = epoch_report\n",
    "\n",
    "\n",
    "        ### TODO: model save or early stopping\n",
    "        if valid_auc > best_auc:\n",
    "            best_auc = valid_auc\n",
    "            best_auc_epoch = epoch + 1\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_acc_epoch = epoch + 1\n",
    "\n",
    "        # scheduler\n",
    "        if args.scheduler == 'plateau':\n",
    "            scheduler.step(best_auc)\n",
    "\n",
    "    # save best records\n",
    "    report['best_auc'] = best_auc\n",
    "    report['best_auc_epoch'] = best_auc_epoch\n",
    "    report['best_acc'] = best_acc\n",
    "    report['best_acc_epoch'] = best_acc_epoch\n",
    "\n",
    "    # save gradient informations\n",
    "    if gradient:\n",
    "        report['gradient'] = args.gradient\n",
    "        del args.gradient\n",
    "        del args['gradient']\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "DATA_PATH = './data'\n",
    "FILE_PATH = 'train_data.csv'\n",
    "# 설정\n",
    "config['seed'] = 42\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['data_dir'] = DATA_PATH\n",
    "\n",
    "# 데이터\n",
    "config['max_seq_len'] = 300\n",
    "\n",
    "# 데이터 증강 (Data Augmentation)\n",
    "config['window'] = False\n",
    "config['stride'] = config['max_seq_len']\n",
    "config['shuffle'] = False\n",
    "config['shuffle_n'] = 2\n",
    "\n",
    "# 모델\n",
    "config['hidden_dim'] = 128\n",
    "config['n_layers'] = 1\n",
    "config['dropout'] = 0.0\n",
    "config['n_heads'] = 4\n",
    "\n",
    "# T Fixup\n",
    "config['Tfixup'] = False\n",
    "config['layer_norm'] = True\n",
    "\n",
    "# 훈련\n",
    "config['n_epochs'] = 10\n",
    "config['batch_size'] = 64\n",
    "config['lr'] = 0.0001\n",
    "config['clip_grad'] = 2.0\n",
    "\n",
    "### 중요 ###\n",
    "config['model'] = 'saint'\n",
    "config['optimizer'] = 'adam'\n",
    "config['scheduler'] = 'plateau'\n",
    "\n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed nan값의 개수: 0\n",
      "KnowledgeTag_percent nan값의 개수: 0\n",
      "cumulative nan값의 개수: 0\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(FILE_PATH)\n",
    "\n",
    "train_data = preprocess.get_train_data()\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련(train) 데이터 준비 완료 : 4688 개\n",
      "검증(valid) 데이터 준비 완료 : 2010 개\n"
     ]
    }
   ],
   "source": [
    "print(f\"훈련(train) 데이터 준비 완료 : {len(train_data)} 개\")\n",
    "print(f\"검증(valid) 데이터 준비 완료 : {len(valid_data)} 개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testID len : 24\n",
      "assessmentItemID len : 24\n",
      "KnowledgeTag size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "target size : 24\n",
      "testID len : 573\n",
      "assessmentItemID len : 573\n",
      "KnowledgeTag size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "target size : 573\n",
      "testID len : 74\n",
      "assessmentItemID len : 74\n",
      "KnowledgeTag size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "target size : 74\n",
      "testID len : 110\n",
      "assessmentItemID len : 110\n",
      "KnowledgeTag size : 110\n",
      "target size : 110\n",
      "target size : 110\n",
      "target size : 110\n",
      "target size : 110\n",
      "target size : 110\n"
     ]
    }
   ],
   "source": [
    "# 배치 단위로 주어지는 데이터를 살펴보자\n",
    "for i,group in enumerate(np.asarray(train_data)):\n",
    "     a,b,c,d,e,f,g,h = np.asarray(group)\n",
    "     print(f\"testID len : {len(a)}\")\n",
    "     print(f\"assessmentItemID len : {len(b)}\")\n",
    "     print(f\"KnowledgeTag size : {len(c)}\")\n",
    "     print(f\"target size : {len(d)}\")\n",
    "     print(f\"target size : {len(e)}\")\n",
    "     print(f\"target size : {len(f)}\")\n",
    "     print(f\"target size : {len(g)}\")\n",
    "     print(f\"target size : {len(h)}\")\n",
    "     if i ==3:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처 : https://www.kaggle.com/bminixhofer/a-validation-framework-impact-of-the-random-seed\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report에서 auc및 실행 시간 정보 얻기\n",
    "def time_auc(report, n_epoch=10):\n",
    "    total_time = 0\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        result = report[str(epoch)]\n",
    "        total_time += result['train_time']\n",
    "        total_time += result['valid_time']\n",
    "\n",
    "    return total_time, report['best_auc'], report['best_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac005c980590475da55570898007ca33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost Time : 134.3642065525055 sec, best AUC : 0.7089434184204289\n"
     ]
    }
   ],
   "source": [
    "# seed 설정\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Gradient 분포도 체크할 것이므로 True로 표시\n",
    "report = run(args, train_data, valid_data, gradient=True)\n",
    "total_time, auc, acc = time_auc(report)\n",
    "\n",
    "print(f\"Cost Time : {total_time} sec, best AUC : {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC : 0.7089434184204289 at epoch 9\n",
      "ACC : 0.6651741293532338 at epoch 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC : {report['best_auc']} at epoch {report['best_auc_epoch']}\")\n",
    "print(f\"ACC : {report['best_acc']} at epoch {report['best_acc_epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
